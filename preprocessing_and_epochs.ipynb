{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4: 2604_2\n",
      "P3: 2604_1\n",
      "P2: 2504_2\n",
      "P2: 2504_1\n",
      "P5: 2704_1\n",
      "P5: 2704_3\n",
      "P7: 2408_1\n",
      "P8: 0305_1\n",
      "P6: 2804_1\n",
      "P1: 2404_2\n",
      "P1: 2404_1\n"
     ]
    }
   ],
   "source": [
    "# 4 - Not connected, 7 - Cable is loose\n",
    "chan_name = ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz','PO8', 'O2']\n",
    "\n",
    "fs, lf, hf = 250, 5, 100 #Hz\n",
    "\n",
    "target_freq = [6.6, 7.5, 13.2, 15, 19.8, 22.5]\n",
    "\n",
    "folder = 'data/Participants/'\n",
    "\n",
    "all_participants = []\n",
    "# iterate  through all the subfolders in the folder\n",
    "for subfolder in os.listdir(folder):\n",
    "    # subfolder total path\n",
    "    subfolder = os.path.join(folder, subfolder)\n",
    "    # if not a folder, skip\n",
    "    if not os.path.isdir(subfolder):\n",
    "        continue\n",
    "    eeg_data = process_folder(subfolder, chan_name, extra_name='path', target_freq=target_freq, hf=hf, lf=lf, epoch_length=6, filter=True)\n",
    "    all_participants.append(eeg_data)\n",
    "\n",
    "for eeg_data in all_participants:\n",
    "    for eeg in eeg_data:\n",
    "        print(eeg.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original channel names: ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz', 'PO8', 'O2']\n",
      "Updated channel names: ['PO3', 'POz', 'PO4', 'O1', 'Oz', 'O2']\n",
      "Original channel names: ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz', 'PO8', 'O2']\n",
      "Updated channel names: ['PO3', 'POz', 'PO4', 'O1', 'Oz', 'O2']\n",
      "Original channel names: ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz', 'PO8', 'O2']\n",
      "Updated channel names: ['PO3', 'POz', 'PO4', 'O1', 'Oz', 'O2']\n",
      "Original channel names: ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz', 'PO8', 'O2']\n",
      "Updated channel names: ['PO3', 'POz', 'PO4', 'O1', 'Oz', 'O2']\n",
      "Original channel names: ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz', 'PO8', 'O2']\n",
      "Updated channel names: ['PO3', 'POz', 'PO4', 'O1', 'Oz', 'O2']\n",
      "Original channel names: ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz', 'PO8', 'O2']\n",
      "Updated channel names: ['PO3', 'POz', 'PO4', 'O1', 'Oz', 'O2']\n",
      "Original channel names: ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz', 'PO8', 'O2']\n",
      "Updated channel names: ['PO3', 'POz', 'PO4', 'O1', 'Oz', 'O2']\n",
      "Original channel names: ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz', 'PO8', 'O2']\n",
      "Updated channel names: ['PO3', 'POz', 'PO4', 'O1', 'Oz', 'O2']\n",
      "Original channel names: ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz', 'PO8', 'O2']\n",
      "Updated channel names: ['PO3', 'POz', 'PO4', 'O1', 'Oz', 'O2']\n",
      "Original channel names: ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz', 'PO8', 'O2']\n",
      "Updated channel names: ['PO3', 'POz', 'PO4', 'O1', 'Oz', 'O2']\n",
      "Original channel names: ['PO3', 'POz', 'PO4', 'PO7', 'O1', 'Oz', 'PO8', 'O2']\n",
      "Updated channel names: ['PO3', 'POz', 'PO4', 'O1', 'Oz', 'O2']\n"
     ]
    }
   ],
   "source": [
    "# sort by participant number\n",
    "all_participants = sorted(all_participants, key=lambda x: int(x[0].title.split(' ')[1]))\n",
    "\n",
    "# remove channels ['PO7', 'PO8']\n",
    "chan_to_remove = ['PO7', 'PO8']\n",
    "\n",
    "for eeg_data in all_participants:\n",
    "    for eeg in eeg_data:\n",
    "        eeg.remove_channels(chan_to_remove)\n",
    "\n",
    "for chan in chan_to_remove:\n",
    "    if chan in chan_name:\n",
    "        chan_name.remove(chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common average re-referencing to remove common noise sources\n",
    "for eeg_data in all_participants:\n",
    "    for eeg in eeg_data:\n",
    "        eeg.filtered_signal = eeg.filtered_signal - np.mean(eeg.filtered_signal, axis=0)\n",
    "\n",
    "    # standardize\n",
    "    for eeg in eeg_data:\n",
    "        mean_val = np.mean(eeg.filtered_signal, axis=0)\n",
    "        std_val = np.std(eeg.filtered_signal, axis=0)\n",
    "        scaled_signal = (eeg.filtered_signal - mean_val) / std_val\n",
    "        # keep only two decimal places\n",
    "        eeg.filtered_signal = np.round(scaled_signal, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'eeg_data' is your list of EEG_Data objects\n",
    "save_folder = 'data/ParticipantsFiltered/'\n",
    "main_folder = 'data/Participants/'\n",
    "\n",
    "# Make the folder if it doesn't exist\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Iterate over each EEG_Data object\n",
    "for eeg_data in all_participants:\n",
    "    for eeg in eeg_data:\n",
    "        # Extract the title from the EEG_Data object\n",
    "        title = eeg.title\n",
    "        # split it and keep only the last part\n",
    "        partic = title[0:2]\n",
    "        act_title = title.split('_')[-1]\n",
    "\n",
    "        # Construct the filename\n",
    "        filename = f'{partic}_{act_title }_ExG.csv'\n",
    "        filepath = os.path.join(save_folder, filename)\n",
    "\n",
    "        # Open a new CSV file in write mode\n",
    "        with open(filepath, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "\n",
    "            # Write the header row with channel names\n",
    "            header = ['TimeStamp'] + eeg.chan_name\n",
    "            writer.writerow(header)\n",
    "\n",
    "            # Write the data rows\n",
    "            for i in range(len(eeg.timestamp)):\n",
    "                timestamp = eeg.timestamp[i]\n",
    "                channel_data = eeg.filtered_signal[:, i]\n",
    "                row = [timestamp] + channel_data.tolist()\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        # Save the markers to the same folder too\n",
    "        markerpath = os.path.join(main_folder, partic)\n",
    "        # find all the files that end with Marker.csv\n",
    "        marker_files = [f for f in os.listdir(markerpath) if f.endswith('_Marker.csv')]\n",
    "        # save them in the same folder\n",
    "        for marker_file in marker_files:\n",
    "            marker_id = marker_file.split('_')[1]\n",
    "            marker_name = f'{partic}_{marker_id}_Marker.csv' \n",
    "            shutil.copy(os.path.join(main_folder, partic, marker_file), os.path.join(save_folder, marker_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_data(marker_path, eeg_path):\n",
    "    # Load marker data\n",
    "    marker_data = pd.read_csv(marker_path)\n",
    "\n",
    "    # remove the sw_ prefix from the Code column\n",
    "    marker_data['Code'] = marker_data['Code'].str.replace('sw_', '')\n",
    "\n",
    "    # Load EEG data\n",
    "    eeg_data = pd.read_csv(eeg_path)\n",
    "\n",
    "    # in eeg_data keep only the rows from the first timestamp to the last timestamp of the marker data\n",
    "    # Get the first and last time stamps from the marker data\n",
    "    first_time_stamp = marker_data['TimeStamp'].iloc[0] - 1 # subtract 1 to make sure we get the first time stamp\n",
    "    last_time_stamp = marker_data['TimeStamp'].iloc[-1] + 1 # add 1 to make sure we get the last time stamp\n",
    "\n",
    "    # boolean masks for the time stamps in eeg_data\n",
    "    greater_than_first = eeg_data['TimeStamp'] >= first_time_stamp\n",
    "    less_than_last = eeg_data['TimeStamp'] <= last_time_stamp\n",
    "\n",
    "    # keep only time stamps between the first and last time stamps in marker_data\n",
    "    eeg_data = eeg_data[greater_than_first & less_than_last]\n",
    "\n",
    "    # Merge EEG data with marker data, add the Code column to the EEG data, based on previous time stamp\n",
    "    merged_data = pd.merge_asof(eeg_data, marker_data, on='TimeStamp', direction='backward')\n",
    "\n",
    "    # group the data by Code\n",
    "    grouped_data = merged_data.groupby('Code')\n",
    "    \n",
    "    return grouped_data\n",
    "\n",
    "def all_grouped_data(data_path):\n",
    "    # Create an empty list to hold the grouped data and participant names\n",
    "    grouped_data_list = []\n",
    "    \n",
    "    # read the files in the subdirectory\n",
    "    participant_files = os.listdir(data_path)\n",
    "\n",
    "    # keep only the csv files\n",
    "    participant_files = [file_name for file_name in participant_files if file_name.endswith('.csv')]\n",
    "\n",
    "    # parse the different names of the files\n",
    "    participant_files = [file_name.split('.')[0] for file_name in participant_files]\n",
    "\n",
    "    # remove the _Marker and _ExG suffixes\n",
    "    participant_files = [file_name.replace('_Marker', '').replace('_ExG', '') for file_name in participant_files]\n",
    "\n",
    "    # remove duplicates\n",
    "    participant_files = list(set(participant_files))\n",
    "\n",
    "    for name in participant_files:\n",
    "        # Load the marker data\n",
    "        marker_file_path = os.path.join(data_path, f\"{name}_Marker.csv\")\n",
    "        # Load the EEG data\n",
    "        eeg_file_path = os.path.join(data_path, f\"{name}_ExG.csv\")\n",
    "        # Group the data\n",
    "        grouped_data = group_data(marker_file_path, eeg_file_path)\n",
    "        # add the participant number to the name\n",
    "        grouped_data_list.append((name, grouped_data))\n",
    "\n",
    "    # sort the list by participant name\n",
    "    grouped_data_list.sort(key=lambda x: x[0])\n",
    "    return grouped_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_path = 'data/ParticipantsFiltered'\n",
    "grouped_data_list = all_grouped_data(filtered_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save each group to a separate file\n",
    "for name, grouped_data in grouped_data_list:\n",
    "    # create a directory for the participant\n",
    "    dir = f\"data/Epochs/\"\n",
    "    if not os.path.exists(dir): os.makedirs(dir)\n",
    "\n",
    "    # save each group to a separate file\n",
    "    for group_name, group_data in grouped_data:\n",
    "        # if group_name starts with 2 or 14, skip it\n",
    "        if group_name.startswith('2') or group_name.startswith('14'): continue\n",
    "        # without the last column Code\n",
    "        group_data.drop(columns=['Code'], inplace=True)\n",
    "        # save the data to a csv file\n",
    "        group_data.to_csv(f\"{dir}/{name}_{group_name}_ExG.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
